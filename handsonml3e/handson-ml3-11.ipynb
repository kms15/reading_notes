{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60cf67a0-1f51-49c4-861f-d0a58d7b037a",
   "metadata": {},
   "source": [
    "# Notes on Chapter 11 of *Hands-On Machine Learning with Scikit-Learn, Keras, & TensorFlow*, 3rd edition, by Aurélien Géron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb41d5-86f8-49b3-a0b0-441b6a8a5a58",
   "metadata": {},
   "source": [
    "Reduce the amount of logging messages displayed by TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d740566-358f-4e43-86e0-950ac3a51ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d4db8a-07cd-45c0-80d1-87b0592cbf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c8d49-3c7e-4041-b80d-68b8d5cc06ce",
   "metadata": {},
   "source": [
    "It's important to choose a reasonable initialization of weights that won't lead to explosion of gradients in either the forward or backpropagation steps.\n",
    "\n",
    "In general, Glorot initialization ($\\sigma^2 = 1/n_{\\textrm{avg}}$) should be used for identity/sigmoid/softmax/tanh activation functions, He initialization ($\\sigma^2 = 2/n_{\\textrm{in}}$) for ReLU derivatives, and LeCun initialization ($\\sigma^2 = 1/n_{\\textrm{in}}$) for SELU.\n",
    "\n",
    "In keras, layer initialization can be specified when creating the layer, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f52eeb9-9a4c-4571-8438-c7c62f2fe9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = tf.keras.layers.Dense(64, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f248caf-88ef-4182-b72f-43d4dd1eb47a",
   "metadata": {},
   "source": [
    "It's also easy to set a custom variance scaling method, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a25e388c-1b8e-4e6e-87ff-fba3e3b234b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_initializer = tf.keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\", distribution=\"uniform\")\n",
    "layer2 = tf.keras.layers.Dense(64, activation=\"leaky_relu\", kernel_initializer=my_initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41012d-6e18-4b3a-8f22-e2ac986b606f",
   "metadata": {},
   "source": [
    "Multiple activation functions are available; the author recommends `relu` or `leaky_relu` for shallow networks, and `swish` ($z \\sigma(\\beta z)$) for deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4305c5-ce2c-4e09-b4b6-9985769c9f3e",
   "metadata": {},
   "source": [
    "Batch normalization is another approach, i.e.\n",
    "\n",
    "$$\n",
    "z_{ij} = \\gamma_j \\frac{ x_{ij} - \\bar x_j }{\\sqrt{\\sigma_j^2 + \\epsilon}} + \\beta_j\n",
    "$$\n",
    "\n",
    "where $\\bar x$ is the vector mean of $x$ (estimated over the batch or as a running average), $\\sigma_j$ is the (estimated) variance of $x_j$, $\\epsilon$ is a smoothing constant (typically $10^{-5}$), and $\\gamma$ and $\\beta$ are (learnable) parameters.\n",
    "\n",
    "Typically training will be slower per epoc due to the extra computation, but convergence will be faster leading to shorter overall training times. Adding batch normalization is as easy as just adding another layer (typically before or after each hidden layer's activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8edd9303-4eb0-4b0f-9cc0-37509166930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3 = tf.keras.layers.BatchNormalization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
